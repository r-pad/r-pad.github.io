
- title: Learning Closed-loop Dough Manipulation using a Differentiable Reset Module
  date: 2022-07-01
  img: ../pics/iros2022_logo.png
  arxiv_url: https://arxiv.org/abs/2207.04638
  pdf: https://arxiv.org/pdf/2207.04638.pdf
  site: https://sites.google.com/view/dough-manipulation/home
  code:
  authors: Carl Qi, Xingyu Lin, David Held
  venue: Robotics and Automation Letters (RA-L) with presentation at the International Conference on Intelligent Robots and Systems (IROS), 2022
  short_id: qi2022dough
  bibtex: "@ARTICLE{9830873,\n
  author={Qi, Carl and Lin, Xingyu and Held, David},\n
  journal={IEEE Robotics and Automation Letters},\n
  title={Learning Closed-Loop Dough Manipulation Using a Differentiable Reset Module},\n
  year={2022},\n
  volume={7},\n
  number={4},\n
  pages={9857-9864},\n
  doi={10.1109/LRA.2022.3191239}}"
  abstract:
    'Deformable object manipulation has many applications such as cooking and laundry folding in our daily lives. Manipulating elastoplastic objects such as dough is particularly challenging because dough lacks a compact state representation and requires contact-rich interactions. We consider the task of flattening a piece of dough into a specific shape from RGB-D images. While the task is seemingly intuitive for humans, there exist local optima for common approaches such as naive trajectory optimization. We propose a novel trajectory optimizer that optimizes through a differentiable "reset" module, transforming a single-stage, fixed-initialization trajectory into a multistage, multi-initialization trajectory where all stages are optimized jointly. We then train a closed-loop policy on the demonstrations generated by our trajectory optimizer. Our policy receives partial point clouds as input, allowing ease of transfer from simulation to the real world. We show that our policy can perform real-world dough manipulation, flattening a ball of dough into a target shape.'
  video_embed:
    <iframe width="560" height="315" src="https://www.youtube.com/embed/b1qKmgmei2U" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

- title: Multi-Modal Transfer Learning for Grasping Transparent and Specular Objects
  date: 2020-02-18
  img: ../pics/transparent_grasping.gif
  arxiv_url: https://arxiv.org/abs/2006.00028
  pdf: https://arxiv.org/pdf/2006.00028.pdf
  site: https://sites.google.com/view/transparent-specular-grasping/home
  code:
  authors: Thomas Weng, Amith Pallankize, Yimin Tang, Oliver Kroemer, David Held
  venue: Robotics and Automation Letters (RA-L) with presentation at the International Conference of Robotics and Automation (ICRA), 2020
  short_id: weng2020ral
  bibtex: "@ARTICLE{9001238, \n
    author={Thomas Weng and Amith Pallankize and Yimin Tang and Oliver Kroemer and David Held}, \n
    journal={IEEE Robotics and Automation Letters}, \n
    title={Multi-Modal Transfer Learning for Grasping Transparent and Specular Objects}, \n
    year={2020}, \n
    volume={5}, \n
    number={3}, \n
    pages={3791-3798}, \n
    doi={10.1109/LRA.2020.2974686}}"
  abstract:
    "State-of-the-art object grasping methods rely on depth sensing to plan robust grasps, but commercially available depth sensors fail to detect transparent and specular objects. To improve grasping performance on such objects, we introduce a method for learning a multi-modal perception model by bootstrapping from an existing uni-modal model. This transfer learning approach requires only a pre-existing uni-modal grasping model and paired multi-modal image data for training, foregoing the need for ground-truth grasp success labels nor real grasp attempts. Our experiments demonstrate that our approach is able to reliably grasp transparent and reflective objects. Video and supplementary material are available at <a href=\"https://sites.google.com/view/transparent-specular-grasping\">https://sites.google.com/view/transparent-specular-grasping</a>."
  video_embed:
    <iframe width="560" height="315" src="https://www.youtube.com/embed/rYRPWe0xLVo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

# - title: Your Project Here
#   date: 2020-03-18
#   arxiv_url: https://arxiv.org/abs/2006.00028
#   pdf: https://arxiv.org/pdf/2006.00028.pdf
#   site: https://sites.google.com/view/transparent-specular-grasping/home
#   code:
#   authors: Thomas Weng, Amith Pallankize, Yimin Tang, Oliver Kroemer, David Held
#   venue: Robotics and Automation Letters (RA-L) with presentation at the International Conference of Robotics and Automation (ICRA), 2020
#   short_id: weng2020ral1
#   bibtex: "@ARTICLE{9001238, \n
#     author={Thomas Weng and Amith Pallankize and Yimin Tang and Oliver Kroemer and David Held}, \n
#     journal={IEEE Robotics and Automation Letters}, \n
#     title={Multi-Modal Transfer Learning for Grasping Transparent and Specular Objects}, \n
#     year={2020}, \n
#     volume={5}, \n
#     number={3}, \n
#     pages={3791-3798}, \n
#     doi={10.1109/LRA.2020.2974686}}"
#   abstract:
#     "State-of-the-art object grasping methods rely on depth sensing to plan robust grasps, but commercially available depth sensors fail to detect transparent and specular objects. To improve grasping performance on such objects, we introduce a method for learning a multi-modal perception model by bootstrapping from an existing uni-modal model. This transfer learning approach requires only a pre-existing uni-modal grasping model and paired multi-modal image data for training, foregoing the need for ground-truth grasp success labels nor real grasp attempts. Our experiments demonstrate that our approach is able to reliably grasp transparent and reflective objects. Video and supplementary material are available at <a href=\"https://sites.google.com/view/transparent-specular-grasping\">https://sites.google.com/view/transparent-specular-grasping</a>."
#   video_embed:
#     <iframe width="560" height="315" src="https://www.youtube.com/embed/rYRPWe0xLVo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
